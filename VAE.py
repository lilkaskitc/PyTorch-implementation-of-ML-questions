# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y26ZOuT96i7uA5hh_8UwDOEBoi54v-Q1
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# -----------------------
# VAE model
# -----------------------
class VAE(nn.Module):
    def __init__(self, input_dim=28*28, hidden_dim=400, latent_dim=20):
        """
        input_dim: flattened input dimension (e.g., 28*28 for MNIST)
        hidden_dim: hidden layer size for encoder/decoder
        latent_dim: dimensionality of z
        """
        super().__init__()
        self.input_dim = input_dim
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

        # decoder
        self.fc_dec = nn.Linear(latent_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """
        x: (B, input_dim)
        returns mu (B, latent_dim), logvar (B, latent_dim)
        """
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """
        Reparameterization trick:
        z = mu + std * eps, eps ~ N(0, I)
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """
        Return logits for Bernoulli reconstruction (use BCEWithLogitsLoss).
        """
        h = F.relu(self.fc_dec(z))
        logits = self.fc_out(h)  # (B, input_dim)
        return logits

    def forward(self, x):
        """
        x expected flattened (B, input_dim)
        returns recon_logits, mu, logvar
        """
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_logits = self.decode(z)
        return recon_logits, mu, logvar

# -----------------------
# Loss: reconstruction + KL
# -----------------------
def vae_loss(recon_logits, x, mu, logvar, reduction='sum'):
    """
    recon_logits: raw decoder outputs (no sigmoid), shape (B, input_dim)
    x: original inputs (B, input_dim) with values in {0,1} or [0,1]
    Using binary cross-entropy with logits (stable).
    KL divergence between q(z|x)=N(mu, var) and p(z)=N(0,I):
      KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))
    Returns total_loss, recon_loss, kl_loss
    """
    # Reconstruction loss (sum over dims then optionally mean over batch)
    bce = F.binary_cross_entropy_with_logits(recon_logits, x, reduction=reduction)
    # KL
    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    if reduction == 'mean':
        return bce + kld, bce, kld
    return bce + kld, bce, kld

# -----------------------
# Utilities: training loop, plotting
# -----------------------
def train_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0.0
    total_bce = 0.0
    total_kld = 0.0
    for x, _ in dataloader:
        x = x.to(device)
        x = x.view(x.size(0), -1)  # flatten
        optimizer.zero_grad()
        recon_logits, mu, logvar = model(x)
        loss, bce, kld = vae_loss(recon_logits, x, mu, logvar, reduction='sum')
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_bce += bce.item()
        total_kld += kld.item()

    n = len(dataloader.dataset)
    return total_loss / n, total_bce / n, total_kld / n

def eval_epoch(model, dataloader, device):
    model.eval()
    total_loss = 0.0
    total_bce = 0.0
    total_kld = 0.0
    with torch.no_grad():
        for x, _ in dataloader:
            x = x.to(device)
            x = x.view(x.size(0), -1)
            recon_logits, mu, logvar = model(x)
            loss, bce, kld = vae_loss(recon_logits, x, mu, logvar, reduction='sum')
            total_loss += loss.item()
            total_bce += bce.item()
            total_kld += kld.item()
    n = len(dataloader.dataset)
    return total_loss / n, total_bce / n, total_kld / n

def sample_and_plot(model, device, n=16):
    model.eval()
    with torch.no_grad():
        z = torch.randn(n, model.fc_mu.out_features).to(device)
        logits = model.decode(z)
        imgs = torch.sigmoid(logits).cpu().view(n, 1, 28, 28).numpy()
    # grid plot
    cols = int(np.sqrt(n))
    fig, axs = plt.subplots(cols, cols, figsize=(cols, cols))
    for i in range(n):
        r = i // cols
        c = i % cols
        axs[r, c].imshow(imgs[i, 0], cmap='gray')
        axs[r, c].axis('off')
    plt.show()

def reconstruct_and_plot(model, dataloader, device, n=8):
    model.eval()
    x, _ = next(iter(dataloader))
    x = x[:n].to(device)
    with torch.no_grad():
        recon_logits, mu, logvar = model(x.view(x.size(0), -1))
        recon = torch.sigmoid(recon_logits).cpu().view(n, 1, 28, 28).numpy()
    orig = x.cpu().view(n, 1, 28, 28).numpy()
    fig, axs = plt.subplots(2, n, figsize=(n, 2))
    for i in range(n):
        axs[0, i].imshow(orig[i, 0], cmap='gray'); axs[0, i].axis('off')
        axs[1, i].imshow(recon[i, 0], cmap='gray'); axs[1, i].axis('off')
    plt.show()

# -----------------------
# Example: train on MNIST
# -----------------------
def run_example(
    epochs=10, batch_size=128, lr=1e-3, latent_dim=20, hidden_dim=400, device=None
):
    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
    transform = transforms.Compose([transforms.ToTensor()])
    train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_ds = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True)

    model = VAE(input_dim=28*28, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(1, epochs+1):
        train_loss, train_bce, train_kld = train_epoch(model, train_loader, optimizer, device)
        test_loss, test_bce, test_kld = eval_epoch(model, test_loader, device)
        print(f"Epoch {epoch:02d} | Train loss {train_loss:.4f} (bce {train_bce:.4f}, kld {train_kld:.4f}) "
              f"| Test loss {test_loss:.4f} (bce {test_bce:.4f}, kld {test_kld:.4f})")

    print("Sampling from prior:")
    sample_and_plot(model, device, n=16)
    print("Reconstruction (top: original, bottom: reconstruction):")
    reconstruct_and_plot(model, test_loader, device, n=8)

    return model

# To run:
# model = run_example(epochs=10)

model = run_example(epochs=10)